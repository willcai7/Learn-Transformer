{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Root Mean Square Layer Normalization\n",
    "\n",
    "**Deliverable**: Implement RMSNorm as a `torch.nn.Module`. To test your implementation against our provided test, you will first need to implement the test adapter at [adapters.run_rmsnorm]. Then, run `pytest -k test_rmsnorm` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,d_model,epsilon=1e-5):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.d_model = d_model\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.epsilon) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Implememnt the position-wise feed-forward network \n",
    "\n",
    "- (a) Deliverable: Implement the GELU activation function. To test your implementation against our provided tests, you will need to implement the test adapter at [adapters.run_gelu]. Then, run `pytest -k test_gelu` to test your implementation.\n",
    "\n",
    "- (b) Deliverable: Implement the position-wise feed-forward network. To test your implememntation, implemement the test adapter at [adpaters.run_poisitonwise_feedforward]. Then, run `pytest -k test_positionwise_feedforward` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FFN, self).__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.activation = GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.w1(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.w2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Implement softmax\n",
    "\n",
    "Deliverable: Write a function to apply the softmax operation on a tensor. Your function should take two parameters: a tensor and a dimension i, and apply softmax to the i-th dimension of the input tensor. The output tensor should have the same shape as the input tensor, but its i-th dimension will now have a normalized probability distribution. Use the same trick as your cross-entropy loss calculation to avoid numerical stability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor, dim: int):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=dim, keepdim=True)[0])\n",
    "    return e_x / e_x.sum(dim=dim, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Implement scaled dot-product attention\n",
    "Deliverable: Implement the scaled dot-product attention function. Your implementation should handle keys and queries of shape `(batch_size, ... , seq_len, d_k)` and values of shape `(batch_size, . . . , seq_len, d_v)`, where . . . represents any number of other batch-like dimensions (if provided). The implementation should return an output with the shape `(batch_size, . . . , d_v)`. See section 3.3 for a discussion on batch-like dimensions.\n",
    "\n",
    "Your implementation should also support an optional user-provided boolean mask of shape `(seq_len, seq_len)`. The attention probabilities of the masked positions should be zero, and the relative probabilities on the non-masked positions should remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attention = softmax(scores, -1)\n",
    "    return torch.matmul(attention, value), attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Implement causal multi-head self-atttention\n",
    "\n",
    "Implement casual multi-head self-attention as a `torch.nn.Module`. You implementation should accept (at least) the following parameters:\n",
    "\n",
    "- `d_model`: `int` Dimensionality of the Transformer block inputs.\n",
    "- `num_heads`: `int` Number of heads to use in multi-head self-attention.\n",
    "- `attn_pdrop`: `float | None = None` Dropout rate for softmax-normalized attention probabilities. \n",
    "\n",
    "Following Vaswani et al, set $d_k = d_v = d_{model} /h$. To test your implementationn against our provided tests, implement the test adapter at [adapters.run_multihead_self_attention]. Then, run `pytest -k test_multihead_self_attention` to test your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads,attn_pdrop=None):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.attention = None\n",
    "        if attn_pdrop is not None:\n",
    "            self.attn_dropout = nn.Dropout(attn_pdrop)\n",
    "        else:\n",
    "            self.attn_dropout = None\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        query = self.query(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = self.key(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.value(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        x, attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        self.attention = attention\n",
    "        x = self.proj(x)\n",
    "        x = self.attn_dropout(x) if self.attn_dropout is not None else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Implement the Transformer block\n",
    "Implement the pre-norm Transformer block as described in ยง3.4 and illustrated in Figure 2. Your Transformer block should accept (at least) the following parameters.\n",
    "\n",
    "- `d_model: int` Dimensionality of the Transformer block inputs.\n",
    "- `num_heads: int` Number of heads to use in multi-head self-attention.\n",
    "- `d_ff: int` Dimensionality of the position-wise feed-forward inner layer.\n",
    "- `attn_pdrop: float | None = None` Dropout rate for softmax-normalized attention probabilities.\n",
    "- `residual_pdrop: float | None = None` Dropout rate for embeddings and Transformer block sublayer outputs.\n",
    "\n",
    "To test your implementation, implement the adapter [adapters.run_transformer_block]. Then run `python -m pytest -k test_transformer_block` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, attn_pdrop=None, residual_pdrop=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads, attn_pdrop)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        self.residual_pdrop = residual_pdrop\n",
    "        self.residual_dropout = nn.Dropout(residual_pdrop)\n",
    "\n",
    "    def foward(self, x):\n",
    "        x = x + self.residual_dropout(self.attn(self.rms_norm(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Implement the Transformer LM\n",
    "Time to put it all together! Implement the Transformer language model as described in ยง3.1 and illustrated in Figure 1. At minimum, your implementation should accept all of the aforementioned construction parameters for the Transformer block, as well as these additional parameters:\n",
    "\n",
    "- `vocab_size: int` The size of the vocabulary, necessary for determining the dimensionality of the token embedding matrix.\n",
    "- `context_length: int` The maximum context length, necessary for determining the dimensionality of the position embedding matrix.\n",
    "- `num_layers: int` The number of Transformer blocks to use.\n",
    "\n",
    "To test your implementation against our provided tests, you will first need to implement the test adapter at [adapters.run_transformer_lm]. Then, run `pytest -k test_transformer_lm` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, context_length, attn_pdrop=None, residual_pdrop=None):\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embeddings = nn.Embedding(context_length, d_model)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, d_ff, attn_pdrop, residual_pdrop) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.drop = nn.Dropout(residual_pdrop)\n",
    "        self.ln_final = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T = x.size()\n",
    "        positions = torch.arange(T, device=x.device, dtype=x.dtype).expand(B, T)\n",
    "        x = self.token_embeddings(x) + self.position_embeddings(positions)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8: Transformer LM resource accounting \n",
    "\n",
    "(a) Consider GPT-2 XL, which has the following configuration:\n",
    "```\n",
    "vocab_size : 50,257  \n",
    "context_length : 1,024  \n",
    "num_layers : 48  \n",
    "d_model : 1,600  \n",
    "num_heads : 25  \n",
    "d_ff : 6,400\n",
    "```\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using single-precision floating point, how much memory is required to just load this model?  \n",
    "\n",
    "Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model. How many FLOPs do these matrix multiplies require in total? Assume that our input sequence has context_length tokens.  \n",
    "\n",
    "(b) Based on your analysis above, which parts of the model require the most FLOPs?\n",
    "\n",
    "(c) Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24 layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). As the model size increases, which parts of the Transformer LM take up proportionally more or less of the total FLOPs?\n",
    "\n",
    "(d) Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one forward pass change? How do the relative contribution of FLOPs of the model components change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory:6.098955869674683GB\n",
      "+--------------------+------------+-----------------------+\n",
      "|        Name        | Parameters |         Ratio         |\n",
      "+--------------------+------------+-----------------------+\n",
      "|       dense        |     0      |          0.0          |\n",
      "|    attention/ln    |    1600    | 9.772926062927871e-07 |\n",
      "|       mlp/ln       |    1600    | 9.772926062927871e-07 |\n",
      "|        ln_f        |    1600    | 9.772926062927871e-07 |\n",
      "| embedding/position |  1638400   | 0.001000747628843814  |\n",
      "|   attention/proj   |  2560000   | 0.0015636681700684594 |\n",
      "|   attention/kqv    |  7680000   | 0.004691004510205378  |\n",
      "|      mlp/ffw       |  10240000  | 0.0062546726802738374 |\n",
      "|      mlp/proj      |  10240000  | 0.0062546726802738374 |\n",
      "|     attention      |  10241600  |  0.00625564997288013  |\n",
      "|        mlp         |  20481600  | 0.012510322653153967  |\n",
      "|       block        |  30723200  |  0.0187659726260341   |\n",
      "|  embedding/token   |  80411200  |  0.0491157945144566   |\n",
      "|      read_out      |  80411200  |  0.0491157945144566   |\n",
      "|     embedding      |  82049600  |  0.05011654214330041  |\n",
      "|    transformer     | 1474713600 |  0.9007666860496367   |\n",
      "|       total        | 1637176000 |          1.0          |\n",
      "+--------------------+------------+-----------------------+\n",
      "+------------------+---------------+-----------------------+\n",
      "|       Name       |     FLOPs     |         Ratio         |\n",
      "+------------------+---------------+-----------------------+\n",
      "| attention/scores |  3355443200   | 0.0009568653688557142 |\n",
      "| attention/reduce |  3355443200   | 0.0009568653688557142 |\n",
      "|  attention/proj  |  5242880000   | 0.0014951021388370535 |\n",
      "|  attention/kqv   |  15728640000  |  0.00448530641651116  |\n",
      "|     mlp/ffw1     |  20971520000  | 0.005980408555348214  |\n",
      "|     mlp/ffw2     |  20971520000  | 0.005980408555348214  |\n",
      "|    attention     |  27682406400  | 0.007894139293059642  |\n",
      "|       mlp        |  41943040000  | 0.011960817110696428  |\n",
      "|      block       |  69625446400  |  0.01985495640375607  |\n",
      "|      dense       | 164682137600  |  0.04696209261970862  |\n",
      "|   transformer    | 3342021427200 |  0.9530379073802914   |\n",
      "|  forward_total   | 3506703564800 |          1.0          |\n",
      "+------------------+---------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "def count_params(\n",
    "    num_decoder_layer: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = n_embd * context_length\n",
    "    out[\"embedding/token\"] = n_embd * vocab_size\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = n_embd  # note, bias=False in our LN\n",
    "    out[\"attention/kqv\"] = n_embd * 3 * n_embd\n",
    "    out[\"attention/proj\"] = n_embd**2\n",
    "    out[\"attention\"] = out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "\n",
    "    # MLP blocks\n",
    "    assert ffw_size == 4 * n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "    out[\"mlp/ln\"] = n_embd\n",
    "    out[\"mlp/ffw\"] = n_embd * ffw_size\n",
    "    out[\"mlp/proj\"] = ffw_size * n_embd\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_layer * out[\"block\"]\n",
    "    out[\"ln_f\"] = n_embd  # final layernorm\n",
    "    out[\"read_out\"] = n_embd * vocab_size\n",
    "    out[\"dense\"] = 0  # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"dense\"] + out[\"read_out\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "params = count_params(\n",
    "    num_decoder_layer=48,\n",
    "    context_length=1024,\n",
    "    n_embd=1600,\n",
    "    ffw_size=6400,\n",
    "    vocab_size=50257,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Total memory:{params['total']*4/1024/1024/1024}GB\")\n",
    "params = OrderedDict(sorted(params.items(), key=lambda x: x[1]))\n",
    "data = {\n",
    "    \"Name\": params.keys(),\n",
    "    \"Parameters\": params.values(),\n",
    "    \"Ratio\": [value/params[\"total\"] for value in params.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------------+\n",
      "|       Name       |     FLOPs     |         Ratio         |\n",
      "+------------------+---------------+-----------------------+\n",
      "| attention/scores |  3355443200   | 0.0009568653688557142 |\n",
      "| attention/reduce |  3355443200   | 0.0009568653688557142 |\n",
      "|  attention/proj  |  5242880000   | 0.0014951021388370535 |\n",
      "|  attention/kqv   |  15728640000  |  0.00448530641651116  |\n",
      "|     mlp/ffw1     |  20971520000  | 0.005980408555348214  |\n",
      "|     mlp/ffw2     |  20971520000  | 0.005980408555348214  |\n",
      "|    attention     |  27682406400  | 0.007894139293059642  |\n",
      "|       mlp        |  41943040000  | 0.011960817110696428  |\n",
      "|      block       |  69625446400  |  0.01985495640375607  |\n",
      "|      dense       | 164682137600  |  0.04696209261970862  |\n",
      "|   transformer    | 3342021427200 |  0.9530379073802914   |\n",
      "|  forward_total   | 3506703564800 |          1.0          |\n",
      "+------------------+---------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "def count_flops(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    n_head: int = 12,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = n_embd // n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * context_length * (n_embd * 3 * n_embd)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * n_head * (context_length * context_length * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * context_length * (n_embd * n_embd)\n",
    "    out[\"attention\"] = sum(out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"])\n",
    "\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * n_embd  # feed forward size\n",
    "    out[\"mlp/ffw1\"] = 2 * context_length * (n_embd * ffw_size)\n",
    "    out[\"mlp/ffw2\"] = 2 * context_length * (ffw_size * n_embd)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"dense\"] = 2 * context_length * (n_embd * vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"dense\"]\n",
    "    # out[\"backward_total\"] = 2 * out[\"forward_total\"]  # use common estimate of bwd = 2*fwd\n",
    "    # out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    out = OrderedDict(sorted(out.items(), key=lambda x: x[1]))\n",
    "    return out\n",
    "\n",
    "flops = count_flops(\n",
    "    num_decoder_blocks=48,\n",
    "    context_length=1024,\n",
    "    n_embd=1600,\n",
    "    n_head=25,\n",
    "    ffw_size=6400,\n",
    "    vocab_size=50257,\n",
    ")\n",
    "\n",
    "flops = OrderedDict(sorted(flops.items(), key=lambda x: x[1]))\n",
    "data = {\n",
    "    \"Name\": flops.keys(),\n",
    "    \"FLOPs\": flops.values(),\n",
    "    \"Ratio\": [value/flops[\"forward_total\"] for value in flops.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+----------------------+\n",
      "|       Name       |    FLOPs     |        Ratio         |\n",
      "+------------------+--------------+----------------------+\n",
      "|  attention/kqv   |  3623878656  | 0.012425508965889174 |\n",
      "| attention/scores |  1610612736  | 0.005522448429284077 |\n",
      "| attention/reduce |  1610612736  | 0.005522448429284077 |\n",
      "|  attention/proj  |  1207959552  | 0.004141836321963058 |\n",
      "|    attention     |  8053063680  | 0.027612242146420385 |\n",
      "|     mlp/ffw1     |  4831838208  | 0.016567345287852232 |\n",
      "|     mlp/ffw2     |  4831838208  | 0.016567345287852232 |\n",
      "|       mlp        |  9663676416  | 0.033134690575704465 |\n",
      "|      block       | 17716740096  | 0.06074693272212485  |\n",
      "|   transformer    | 212600881152 |  0.7289631926654981  |\n",
      "|      dense       | 79047426048  |  0.2710368073345018  |\n",
      "|  forward_total   | 291648307200 |         1.0          |\n",
      "+------------------+--------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 small \n",
    "\n",
    "n_layer = 12\n",
    "d_model = 768\n",
    "n_head = 12\n",
    "d_ff = 3072\n",
    "\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "\n",
    "# params = count_params(n_layer, context_length, d_model, d_ff, vocab_size)\n",
    "flops = count_flops(n_layer, context_length, d_model, n_head, d_ff, vocab_size)\n",
    "\n",
    "data = {\n",
    "    \"Name\": flops.keys(),\n",
    "    \"FLOPs\": flops.values(),\n",
    "    \"Ratio\": [value/flops[\"forward_total\"] for value in flops.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-----------------------+\n",
      "|       Name       |    FLOPs     |         Ratio         |\n",
      "+------------------+--------------+-----------------------+\n",
      "|  attention/kqv   |  6442450944  | 0.0077906071449402895 |\n",
      "| attention/scores |  2147483648  |  0.00259686904831343  |\n",
      "| attention/reduce |  2147483648  |  0.00259686904831343  |\n",
      "|  attention/proj  |  2147483648  |  0.00259686904831343  |\n",
      "|    attention     | 12884901888  | 0.015581214289880579  |\n",
      "|     mlp/ffw1     |  8589934592  |  0.01038747619325372  |\n",
      "|     mlp/ffw2     |  8589934592  |  0.01038747619325372  |\n",
      "|       mlp        | 17179869184  |  0.02077495238650744  |\n",
      "|      block       | 30064771072  |  0.03635616667638802  |\n",
      "|   transformer    | 721554505728 |  0.8725480002333125   |\n",
      "|      dense       | 105396568064 |  0.12745199976668756  |\n",
      "|  forward_total   | 826951073792 |          1.0          |\n",
      "+------------------+--------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 medium \n",
    "\n",
    "n_layer = 24\n",
    "d_model = 1024\n",
    "n_head = 16\n",
    "d_ff = d_model * 4\n",
    "\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "\n",
    "# params = count_params(n_layer, context_length, d_model, d_ff, vocab_size)\n",
    "flops = count_flops(n_layer, context_length, d_model, n_head, d_ff, vocab_size)\n",
    "\n",
    "data = {\n",
    "    \"Name\": flops.keys(),\n",
    "    \"FLOPs\": flops.values(),\n",
    "    \"Ratio\": [value/flops[\"forward_total\"] for value in flops.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------------+\n",
      "|       Name       |     FLOPs     |         Ratio         |\n",
      "+------------------+---------------+-----------------------+\n",
      "|  attention/kqv   |  10066329600  | 0.0056725435596688065 |\n",
      "| attention/scores |  2684354560   | 0.0015126782825783482 |\n",
      "| attention/reduce |  2684354560   | 0.0015126782825783482 |\n",
      "|  attention/proj  |  3355443200   | 0.0018908478532229354 |\n",
      "|    attention     |  18790481920  | 0.010588747978048438  |\n",
      "|     mlp/ffw1     |  13421772800  | 0.007563391412891742  |\n",
      "|     mlp/ffw2     |  13421772800  | 0.007563391412891742  |\n",
      "|       mlp        |  26843545600  | 0.015126782825783483  |\n",
      "|      block       |  45634027520  |  0.02571553080383192  |\n",
      "|   transformer    | 1642824990720 |  0.9257591089379492   |\n",
      "|      dense       | 131745710080  |  0.07424089106205083  |\n",
      "|  forward_total   | 1774570700800 |          1.0          |\n",
      "+------------------+---------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 large \n",
    "\n",
    "n_layer = 36\n",
    "d_model = 1280\n",
    "n_head = 20\n",
    "d_ff = 1280*4\n",
    "\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "\n",
    "# params = count_params(n_layer, context_length, d_model, d_ff, vocab_size)\n",
    "flops = count_flops(n_layer, context_length, d_model, n_head, d_ff, vocab_size)\n",
    "\n",
    "data = {\n",
    "    \"Name\": flops.keys(),\n",
    "    \"FLOPs\": flops.values(),\n",
    "    \"Ratio\": [value/flops[\"forward_total\"] for value in flops.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------------------+\n",
      "|       Name       |      FLOPs      |         Ratio         |\n",
      "+------------------+-----------------+-----------------------+\n",
      "|  attention/kqv   |  251658240000   | 0.0018921053119957884 |\n",
      "| attention/scores |  858993459200   | 0.006458386131612291  |\n",
      "| attention/reduce |  850403524608   | 0.006393802270296168  |\n",
      "|  attention/proj  |   83886080000   | 0.0006307017706652628 |\n",
      "|    attention     |  2044941303808  |  0.01537499548456951  |\n",
      "|     mlp/ffw1     |  335544320000   | 0.0025228070826610514 |\n",
      "|     mlp/ffw2     |  335544320000   | 0.0025228070826610514 |\n",
      "|       mlp        |  671088640000   | 0.005045614165322103  |\n",
      "|      block       |  2716029943808  | 0.020420609649891612  |\n",
      "|   transformer    | 130369437302784 |  0.9801892631947974   |\n",
      "|      dense       |  2634914201600  |  0.01981073680520257  |\n",
      "|  forward_total   | 133004351504384 |          1.0          |\n",
      "+------------------+-----------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 XL \n",
    "\n",
    "n_layer = 48\n",
    "d_model = 1600\n",
    "n_head = 48\n",
    "d_ff = d_model*4\n",
    "\n",
    "vocab_size = 50257\n",
    "context_length = 16384\n",
    "\n",
    "# params = count_params(n_layer, context_length, d_model, d_ff, vocab_size)\n",
    "flops = count_flops(n_layer, context_length, d_model, n_head, d_ff, vocab_size)\n",
    "\n",
    "data = {\n",
    "    \"Name\": flops.keys(),\n",
    "    \"FLOPs\": flops.values(),\n",
    "    \"Ratio\": [value/flops[\"forward_total\"] for value in flops.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
