{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems for BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding Unicode\n",
    "- (a) What Unicode character does chr(0) return?  \n",
    "    Deliverable: A one-sentence response.  \n",
    "    '\\x00', space.\n",
    "- (b) How does this character’s string representation (__repr__()) differ from its printed representation?  \n",
    "    Deliverable: A one-sentence response.  \n",
    "    \"'\\x00'\", \" \"\n",
    "- (c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations: \n",
    "\n",
    "    Deliverable: A one-sentence response.\n",
    "    ```python\n",
    "    >>> chr(0) \n",
    "    >>> print(chr(0)) \n",
    "    >>> \"this is a test\" + chr(0) + \"string\"\n",
    "    >>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "    ```\n",
    "\n",
    "    In string, it's '\\x00'. If we print the string, it becomes space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0).__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Unicode Encodings\n",
    "- (a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "    - Vocabulary Size: UTF-8 limits the vocabulary to 256 possible tokens, making the model simpler.\n",
    "    - Efficiency: For texts with lots of ASCII characters, UTF-8 is much more space- and compute-efficient.\n",
    "    - Simplicity in Tokenization: Handling a stream of bytes is simpler than dealing with variable-length code units or surrogate pairs\n",
    "    - Robustness: Byte-level models trained on UTF-8 can better handle noisy or unexpected inputs.\n",
    "    -Compatibility: UTF-8’s ubiquity makes it easier to integrate with various data sources and systems.\n",
    "- (b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.  \n",
    "    ```python\n",
    "    def decode_utf8_bytes_to_str_wrong(bytestring: bytes): \n",
    "        return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])  \n",
    "    >>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")) \n",
    "    'hello'  \n",
    "    ``` \n",
    "    One token can has multiple bytes. For example \"蔡\".\n",
    "- (c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "    b'\\xe8\\x94'. Typically, they have some special bytes for start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe8\\x94\\xa1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\xe8\\x94'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes): return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "print(\"蔡\".encode(\"utf-8\"))\n",
    "b'\\xe8\\x94'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: BPE Tokenizer Training\n",
    "Deliverable: Write a function that, given path to an input text file, trains a (byte-level) BPE tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "- input_path: str Path to a text file with BPE tokenizer training data.  \n",
    "\n",
    "- vocab_size: int A non-negative integer that defines the maximum final vocabulary size (including the initial byte vocabulary, vocabulary items produced from merging, and any special tokens).  \n",
    "\n",
    "- special_tokens: list[str] A list of strings to add to the vocabulary. These special tokens do not otherwise affect BPE training.  \n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:  \n",
    "\n",
    "- vocab: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabulary) to bytes (token bytes).  \n",
    "\n",
    "- merges: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with <token2>. The merges should be ordered by order of creation.  \n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at [adapters.run_train_bpe]. Then, run pytest tests/test_train_bpe.py. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re \n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the code for BPE, we construct a bi-direct list, which will be very useful for the bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilist:\n",
    "    def __init__(self, val, id, previous=None, next=None):\n",
    "        self.val = val \n",
    "        self.id = id \n",
    "        self.prev = previous\n",
    "        self.next = next \n",
    "    \n",
    "    def merge(self):\n",
    "        if self.next is None:\n",
    "            raise ValueError(\"Cannot merge last element\")\n",
    "        else:\n",
    "            self.val = self.val + self.next.val\n",
    "            self.next = self.next.next\n",
    "            if self.next:\n",
    "                self.next.prev = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to write the prototype for the BPE algorithm. We consider some trivial cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'low', 'low', 'low', 'low', 'lower', 'lower', 'widest', 'widest', 'widest', 'newest', 'newest', 'newest', 'newest', 'newest', 'newest']\n",
      "[(b's', b't'), (b'e', b'st'), (b'o', b'w'), (b'l', b'ow'), (b'w', b'est'), (b'n', b'e')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "text = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest\" # text to be tokenized\n",
    "prim_tokens = text.split() # split text into tokens\n",
    "print(prim_tokens) # print tokens\n",
    "\n",
    "merges = [] # list of merges, tuple[bytes, bytes]\n",
    "vocab = {} # int to  bytes dictionary [int: bytes]\n",
    "\n",
    "id_to_token = {} # id to token dictionary\n",
    "token_to_id = {} # token to id dictionary\n",
    "frequency_tokens = {} # frequency of tokens table\n",
    "frequency_pairs = {}  # frequency of pairs table\n",
    "positions_pairs = defaultdict(list) # positions of pairs table, position is a bilist object\n",
    "\n",
    "\n",
    "# create token to id, id to token dictionaries, frequency_tokens table, and positions_pairs table\n",
    "for prim_token in prim_tokens:\n",
    "    chars = [bytes([b]) for b in prim_token.encode(\"utf-8\")] # get bytes of token\n",
    "    if prim_token in token_to_id:\n",
    "        frequency_tokens[token_to_id[prim_token]] += 1 # increment frequency of token\n",
    "    else:\n",
    "        id = len(token_to_id) # get new id\n",
    "        token_to_id[prim_token] = id # add token to id dictionary\n",
    "        id_to_token[id] = prim_token # add id to token dictionary\n",
    "        frequency_tokens[id] = 1 # set frequency of token to 1\n",
    "\n",
    "        bi_char = bilist(chars[0], id) # create first bilist object\n",
    "        for char in chars[1:]:\n",
    "            new_bi_char = bilist(char, id, bi_char) # create new bilist object\n",
    "            bi_char.next = new_bi_char # link new bilist object to previous one\n",
    "            pairs = (bi_char.val, new_bi_char.val) # create pair\n",
    "            positions_pairs[pairs].append(bi_char) # add position to pair\n",
    "            bi_char = new_bi_char # move to next bilist object\n",
    "\n",
    "# update the frequency_pairs table\n",
    "for pairs, positions in positions_pairs.items(): \n",
    "    for position in positions:\n",
    "        frequency_pairs[pairs] = frequency_pairs.get(pairs, 0) + frequency_tokens[position.id]  # increment frequency of pair\n",
    "\n",
    "# print(frequency_pairs)\n",
    "\n",
    "# find the pair with the highest frequency and marge \n",
    "for _ in range(6):\n",
    "    \n",
    "    max_pair = max(frequency_pairs, key=lambda x:(frequency_pairs[x], x)) # get pair with highest frequency\n",
    "    # print(max_pair) # print pair\n",
    "    merges.append(max_pair) # add pair to merges list\n",
    "    positions = positions_pairs[max_pair] # get positions of pair\n",
    "    \n",
    "    for position in positions: \n",
    "\n",
    "        val1 = position.val # get value of position\n",
    "        val2 = position.next.val # get value of next position\n",
    "        temp_pos =  position.next # get next position\n",
    "        position.merge() # merge position with next position\n",
    "\n",
    "        if position.prev: # if there is a previous position\n",
    "            prev_pair = (position.prev.val, val1)\n",
    "            frequency_pairs[prev_pair] -= frequency_tokens[position.id] # decrement frequency of pair   \n",
    "            positions_pairs[prev_pair].remove(position.prev) # remove previous position from pair\n",
    "            new_prev_pair = (position.prev.val, position.val) # create new pair\n",
    "            frequency_pairs[new_prev_pair] = frequency_pairs.get(new_prev_pair, 0) + frequency_tokens[position.id] # increment frequency of new pair\n",
    "            positions_pairs[new_prev_pair].append(position.prev) # add previous position to new pair\n",
    "\n",
    "        if position.next: # if there is a next position\n",
    "            next_pair = (val2, position.next.val)\n",
    "            frequency_pairs[next_pair] -= frequency_tokens[position.id] # decrement frequency of pair\n",
    "            positions_pairs[next_pair].remove(temp_pos) # remove next position from pair\n",
    "            new_next_pair = (position.val, position.next.val) # create new pair\n",
    "            frequency_pairs[new_next_pair] = frequency_pairs.get(new_next_pair, 0) + frequency_tokens[position.id] # increment frequency of new pair\n",
    "            positions_pairs[new_next_pair].append(position) # add position to new pair  \n",
    "\n",
    "    del positions_pairs[max_pair] # delete pair from positions_pairs\n",
    "    del frequency_pairs[max_pair] # delete pair from frequency_pairs\n",
    "\n",
    "    # print(frequency_pairs)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for now, this code is almost working. The remaining thing is to implement this for the general text. It's worth noting that for the general case, there is a special token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE model\n",
      "Tokenizing text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|██████████| 1015/1015 [00:00<00:00, 101500.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vocab\n",
      "Reading tokens and creating bilist objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26743/26743 [00:00<00:00, 122575.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating frequency pairs table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1069/1069 [00:00<00:00, 213825.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding merges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:00<00:00, 2585.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'<|endoftext|>', 257: b' t', 258: b' a', 259: b'he', 260: b'in', 261: b' the', 262: b're', 263: b' o', 264: b' ,', 265: b'er', 266: b' s', 267: b'at', 268: b' .', 269: b'nd', 270: b'is', 271: b'or', 272: b' w', 273: b' c', 274: b'on', 275: b' b', 276: b' f', 277: b'ou', 278: b'it', 279: b'en', 280: b'es', 281: b' of', 282: b' p', 283: b'ing', 284: b' in', 285: b'ed', 286: b'al', 287: b' m', 288: b' and', 289: b' d', 290: b'an', 291: b'ar', 292: b' to', 293: b'om', 294: b' th', 295: b'ic', 296: b'ion', 297: b' h', 298: b' l', 299: b' y', 300: b' e', 301: b'as', 302: b'ot', 303: b'il', 304: b' n', 305: b' u', 306: b'ent', 307: b' be', 308: b' &', 309: b' is', 310: b' you', 311: b'os', 312: b' re', 313: b'et', 314: b' for', 315: b'ut', 316: b'el', 317: b' g', 318: b'ay', 319: b'st', 320: b'ow', 321: b'le', 322: b'ce', 323: b'ad', 324: b' on', 325: b' I', 326: b'ver', 327: b've', 328: b' A', 329: b'ur', 330: b'ol', 331: b'ct', 332: b'qu', 333: b' that', 334: b'im', 335: b'all', 336: b'am', 337: b'ig', 338: b'ch', 339: b'ation', 340: b' P', 341: b'ith', 342: b'ir', 343: b' S', 344: b' it', 345: b' pr', 346: b'ap', 347: b' sh', 348: b' C', 349: b'th', 350: b' com', 351: b' @', 352: b' wh', 353: b'-@', 354: b' are', 355: b' @-@', 356: b'nt', 357: b'id', 358: b' with', 359: b' al', 360: b'op', 361: b' us', 362: b'ers', 363: b' as', 364: b'the', 365: b'and', 366: b'if', 367: b'ord', 368: b'od', 369: b' he', 370: b'ist', 371: b'quot', 372: b'ment', 373: b' M', 374: b' or', 375: b'ore', 376: b' G', 377: b' fr', 378: b'ill', 379: b'res', 380: b' st', 381: b'ess', 382: b'ld', 383: b' this', 384: b' 2', 385: b'art', 386: b' ;', 387: b' L', 388: b'ly', 389: b'ain', 390: b'ul', 391: b' de', 392: b' con', 393: b'est', 394: b'se', 395: b'apos', 396: b'ag', 397: b' from', 398: b' an', 399: b' we', 400: b' (', 401: b'00', 402: b'ter', 403: b' E', 404: b'em', 405: b'ave', 406: b' not', 407: b' )', 408: b' 1', 409: b' your', 410: b'oc', 411: b' can', 412: b' by', 413: b' D', 414: b' ne', 415: b' v', 416: b'igh', 417: b'ich', 418: b' all', 419: b'ri', 420: b' up', 421: b' r', 422: b' W', 423: b'ble', 424: b' they', 425: b' B', 426: b'un', 427: b' ye', 428: b' which', 429: b' O', 430: b'ke', 431: b' wor', 432: b' su', 433: b' F', 434: b' H', 435: b' have', 436: b'ate', 437: b' shall', 438: b' ch', 439: b'ect', 440: b'ity', 441: b' sp', 442: b'ress', 443: b'ight', 444: b' will', 445: b' comp', 446: b'ort', 447: b'ant', 448: b' &#', 449: b'ive', 450: b'are', 451: b'..', 452: b' ex', 453: b' And', 454: b' ...', 455: b'ast', 456: b'24', 457: b' T', 458: b'ould', 459: b'ven', 460: b' tr', 461: b'ust', 462: b'um', 463: b'out', 464: b'com', 465: b' unt', 466: b' se', 467: b'ft', 468: b'ree', 469: b'ost', 470: b'og', 471: b'ish', 472: b'ions', 473: b'iz', 474: b'124', 475: b' unto', 476: b'mer', 477: b'ings', 478: b' ac', 479: b'this', 480: b'ated', 481: b'ac', 482: b'lu', 483: b'ere', 484: b' man', 485: b'for', 486: b' my', 487: b' at', 488: b'ies', 489: b'age', 490: b'rou', 491: b'lo', 492: b'ans', 493: b'pp', 494: b'ind', 495: b' work', 496: b'here', 497: b'fore', 498: b' sit', 499: b' ver'}\n",
      "[(b' ', b't'), (b' ', b'a'), (b'h', b'e'), (b'i', b'n'), (b' t', b'he'), (b'r', b'e'), (b' ', b'o'), (b' ', b','), (b'e', b'r'), (b' ', b's'), (b'a', b't'), (b' ', b'.'), (b'n', b'd'), (b'i', b's'), (b'o', b'r'), (b' ', b'w'), (b' ', b'c'), (b'o', b'n'), (b' ', b'b'), (b' ', b'f'), (b'o', b'u'), (b'i', b't'), (b'e', b'n'), (b'e', b's'), (b' o', b'f'), (b' ', b'p'), (b'in', b'g'), (b' ', b'in'), (b'e', b'd'), (b'a', b'l'), (b' ', b'm'), (b' a', b'nd'), (b' ', b'd'), (b'a', b'n'), (b'a', b'r'), (b' t', b'o'), (b'o', b'm'), (b' t', b'h'), (b'i', b'c'), (b'i', b'on'), (b' ', b'h'), (b' ', b'l'), (b' ', b'y'), (b' ', b'e'), (b'a', b's'), (b'o', b't'), (b'i', b'l'), (b' ', b'n'), (b' ', b'u'), (b'en', b't'), (b' b', b'e'), (b' ', b'&'), (b' ', b'is'), (b' y', b'ou'), (b'o', b's'), (b' ', b're'), (b'e', b't'), (b' f', b'or'), (b'u', b't'), (b'e', b'l'), (b' ', b'g'), (b'a', b'y'), (b's', b't'), (b'o', b'w'), (b'l', b'e'), (b'c', b'e'), (b'a', b'd'), (b' o', b'n'), (b' ', b'I'), (b'v', b'er'), (b'v', b'e'), (b' ', b'A'), (b'u', b'r'), (b'o', b'l'), (b'c', b't'), (b'q', b'u'), (b' th', b'at'), (b'i', b'm'), (b'al', b'l'), (b'a', b'm'), (b'i', b'g'), (b'c', b'h'), (b'at', b'ion'), (b' ', b'P'), (b'it', b'h'), (b'i', b'r'), (b' ', b'S'), (b' ', b'it'), (b' p', b'r'), (b'a', b'p'), (b' s', b'h'), (b' ', b'C'), (b't', b'h'), (b' c', b'om'), (b' ', b'@'), (b' w', b'h'), (b'-', b'@'), (b' a', b're'), (b' @', b'-@'), (b'n', b't'), (b'i', b'd'), (b' w', b'ith'), (b' a', b'l'), (b'o', b'p'), (b' u', b's'), (b'er', b's'), (b' a', b's'), (b't', b'he'), (b'a', b'nd'), (b'i', b'f'), (b'or', b'd'), (b'o', b'd'), (b' ', b'he'), (b'is', b't'), (b'qu', b'ot'), (b'm', b'ent'), (b' ', b'M'), (b' o', b'r'), (b'o', b're'), (b' ', b'G'), (b' f', b'r'), (b'il', b'l'), (b're', b's'), (b' s', b't'), (b'es', b's'), (b'l', b'd'), (b' th', b'is'), (b' ', b'2'), (b'ar', b't'), (b' ', b';'), (b' ', b'L'), (b'l', b'y'), (b'a', b'in'), (b'u', b'l'), (b' d', b'e'), (b' c', b'on'), (b'es', b't'), (b's', b'e'), (b'ap', b'os'), (b'a', b'g'), (b' fr', b'om'), (b' a', b'n'), (b' w', b'e'), (b' ', b'('), (b'0', b'0'), (b't', b'er'), (b' ', b'E'), (b'e', b'm'), (b'a', b've'), (b' n', b'ot'), (b' ', b')'), (b' ', b'1'), (b' you', b'r'), (b'o', b'c'), (b' c', b'an'), (b' b', b'y'), (b' ', b'D'), (b' n', b'e'), (b' ', b'v'), (b'ig', b'h'), (b'ic', b'h'), (b' al', b'l'), (b'r', b'i'), (b' u', b'p'), (b' ', b'r'), (b' ', b'W'), (b'b', b'le'), (b' the', b'y'), (b' ', b'B'), (b'u', b'n'), (b' y', b'e'), (b' wh', b'ich'), (b' ', b'O'), (b'k', b'e'), (b' w', b'or'), (b' s', b'u'), (b' ', b'F'), (b' ', b'H'), (b' h', b'ave'), (b'at', b'e'), (b' sh', b'all'), (b' c', b'h'), (b'e', b'ct'), (b'it', b'y'), (b' s', b'p'), (b'res', b's'), (b'igh', b't'), (b' w', b'ill'), (b' com', b'p'), (b'or', b't'), (b'an', b't'), (b' &', b'#'), (b'i', b've'), (b'a', b're'), (b'.', b'.'), (b' e', b'x'), (b' A', b'nd'), (b' .', b'..'), (b'as', b't'), (b'2', b'4'), (b' ', b'T'), (b'ou', b'ld'), (b'v', b'en'), (b' t', b'r'), (b'u', b'st'), (b'u', b'm'), (b'ou', b't'), (b'c', b'om'), (b' u', b'nt'), (b' s', b'e'), (b'f', b't'), (b're', b'e'), (b'os', b't'), (b'o', b'g'), (b'is', b'h'), (b'ion', b's'), (b'i', b'z'), (b'1', b'24'), (b' unt', b'o'), (b'm', b'er'), (b'ing', b's'), (b' a', b'c'), (b'th', b'is'), (b'at', b'ed'), (b'a', b'c'), (b'l', b'u'), (b'e', b're'), (b' m', b'an'), (b'f', b'or'), (b' m', b'y'), (b' a', b't'), (b'i', b'es'), (b'ag', b'e'), (b'r', b'ou'), (b'l', b'o'), (b'an', b's'), (b'p', b'p'), (b'in', b'd'), (b' wor', b'k'), (b'he', b're'), (b'f', b'ore'), (b' s', b'it'), (b' ', b'ver')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class bilist:\n",
    "    def __init__(self, val, id, previous=None, next=None):\n",
    "        self.val = val \n",
    "        self.id = id \n",
    "        self.prev = previous\n",
    "        self.next = next \n",
    "    \n",
    "    def merge(self):\n",
    "        if self.next is None:\n",
    "            raise ValueError(\"Cannot merge last element\")\n",
    "        else:\n",
    "            self.val = self.val + self.next.val\n",
    "            self.next = self.next.next\n",
    "            if self.next:\n",
    "                self.next.prev = self\n",
    "\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\" # pattern for tokenization\n",
    "def train_bpe(input_path:str, vocab_size:int, special_tokens:list[str]):\n",
    "    \"\"\" \n",
    "    Train a BPE model on a text file.\n",
    "    \n",
    "    Args:\n",
    "\n",
    "    input_path: str\n",
    "        The path to the input text file.\n",
    "    vocab_size: int\n",
    "        The size of the vocabulary.\n",
    "    special_tokens: list[str]\n",
    "        A list of special tokens.\n",
    "    \n",
    "    Returns:\n",
    "    vocab: dict[int, bytes]\n",
    "        A dictionary mapping token ids to tokens.\n",
    "    merges: list[tuple[bytes, bytes]]\n",
    "        A list of merges.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Training BPE model\")\n",
    "    print(\"Tokenizing text\")\n",
    "    with open(input_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Split text into lines for progress monitoring\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Initialize an empty list to store tokens\n",
    "    prim_tokens = []\n",
    "\n",
    "    # Process each line and update the progress bar\n",
    "    for line in tqdm(lines, desc=\"Processing text\"):\n",
    "        tokens = re.findall(PAT, line)\n",
    "        prim_tokens.extend(tokens)\n",
    "\n",
    "    # Initialize the vocab with 256 bytes and sepcial tokens\n",
    "    print(\"Initializing vocab\")\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    for i, token in enumerate(special_tokens):\n",
    "        vocab[256+i] = token.encode(\"utf-8\")\n",
    "    \n",
    "    merges = [] # list of merges, tuple[bytes, bytes]\n",
    "    id_to_token = {} # id to token dictionary\n",
    "    token_to_id = {} # token to id dictionary\n",
    "    frequency_tokens = {} # frequency of tokens table\n",
    "    frequency_pairs = {}  # frequency of pairs table\n",
    "    positions_pairs = defaultdict(list) # positions of pairs table, position is a bilist object\n",
    "\n",
    "\n",
    "    # create token to id, id to token dictionaries, frequency_tokens table, and positions_pairs table\n",
    "\n",
    "    print(\"Reading tokens and creating bilist objects\")\n",
    "    for prim_token in tqdm(prim_tokens):\n",
    "        chars = [bytes([b]) for b in prim_token.encode(\"utf-8\")] # get bytes of token\n",
    "        if prim_token in token_to_id:\n",
    "            frequency_tokens[token_to_id[prim_token]] += 1 # increment frequency of token\n",
    "        else:\n",
    "            id = len(token_to_id) # get new id\n",
    "            token_to_id[prim_token] = id # add token to id dictionary\n",
    "            id_to_token[id] = prim_token # add id to token dictionary\n",
    "            frequency_tokens[id] = 1 # set frequency of token to 1\n",
    "\n",
    "            bi_char = bilist(chars[0], id) # create first bilist object\n",
    "            for char in chars[1:]:\n",
    "                new_bi_char = bilist(char, id, bi_char) # create new bilist object\n",
    "                bi_char.next = new_bi_char # link new bilist object to previous one\n",
    "                pairs = (bi_char.val, new_bi_char.val) # create pair\n",
    "                positions_pairs[pairs].append(bi_char) # add position to pair\n",
    "                bi_char = new_bi_char # move to next bilist object\n",
    "\n",
    "    # update the frequency_pairs table\n",
    "    print(\"Updating frequency pairs table\")\n",
    "    for pairs, positions in tqdm(positions_pairs.items()): \n",
    "        for position in positions:\n",
    "            frequency_pairs[pairs] = frequency_pairs.get(pairs, 0) + frequency_tokens[position.id]  # increment frequency of pair\n",
    "\n",
    "    # print(frequency_pairs)\n",
    "\n",
    "    # find the pair with the highest frequency and marge \n",
    "    # while len(vocab) < vocab_size:\n",
    "    print(\"Finding merges\")\n",
    "    for _ in tqdm(range(vocab_size - len(vocab))):\n",
    "            \n",
    "        max_pair = max(frequency_pairs, key=lambda x:(frequency_pairs[x], x)) # get pair with highest frequency\n",
    "        # print(max_pair) # print pair\n",
    "        merges.append(max_pair) # add pair to merges list\n",
    "        new_char = max_pair[0] + max_pair[1] # create new character\n",
    "        vocab[len(vocab)] = new_char # add new character to vocab\n",
    "        positions = positions_pairs[max_pair] # get positions of pair\n",
    "        \n",
    "        for position in positions: \n",
    "\n",
    "            val1 = position.val # get value of position\n",
    "            val2 = position.next.val # get value of next position\n",
    "            temp_pos =  position.next # get next position\n",
    "            position.merge() # merge position with next position\n",
    "\n",
    "            if position.prev: # if there is a previous position\n",
    "                prev_pair = (position.prev.val, val1)\n",
    "                frequency_pairs[prev_pair] -= frequency_tokens[position.id] # decrement frequency of pair   \n",
    "                positions_pairs[prev_pair].remove(position.prev) # remove previous position from pair\n",
    "                new_prev_pair = (position.prev.val, position.val) # create new pair\n",
    "                frequency_pairs[new_prev_pair] = frequency_pairs.get(new_prev_pair, 0) + frequency_tokens[position.id] # increment frequency of new pair\n",
    "                positions_pairs[new_prev_pair].append(position.prev) # add previous position to new pair\n",
    "\n",
    "            if position.next: # if there is a next position\n",
    "                next_pair = (val2, position.next.val)\n",
    "                frequency_pairs[next_pair] -= frequency_tokens[position.id] # decrement frequency of pair\n",
    "                positions_pairs[next_pair].remove(temp_pos) # remove next position from pair\n",
    "                new_next_pair = (position.val, position.next.val) # create new pair\n",
    "                frequency_pairs[new_next_pair] = frequency_pairs.get(new_next_pair, 0) + frequency_tokens[position.id] # increment frequency of new pair\n",
    "                positions_pairs[new_next_pair].append(position) # add position to new pair  \n",
    "\n",
    "        del positions_pairs[max_pair] # delete pair from positions_pairs\n",
    "        del frequency_pairs[max_pair] # delete pair from frequency_pairs\n",
    "    \n",
    "    return vocab, merges\n",
    "\n",
    "vocab, merges = train_bpe(\"./data/fixtures/corpus.en\", 500, [\"<|endoftext|>\"])\n",
    "print(vocab)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: BPE training on TinyStories \n",
    "- (a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?  \n",
    "Resource requirements: ≤ 30 minutes (no GPUs), ≤ 30GB RAM \n",
    "\n",
    "- (b) Profile your code. What part of the tokenizer training process takes the most time?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE model\n",
      "Tokenizing text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|██████████| 15600063/15600063 [03:18<00:00, 78537.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vocab\n",
      "Reading tokens and creating bilist objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529467880/529467880 [06:24<00:00, 1377433.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating frequency pairs table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2098/2098 [00:00<00:00, 24976.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding merges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9743/9743 [00:49<00:00, 198.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before: 8579.41015625 MB\n",
      "Memory after: 1245.51953125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import psutil \n",
    "import time \n",
    "\n",
    "process = psutil.Process()\n",
    "mem_before = process.memory_info().rss/1024/1024\n",
    "path_file = \"./data/TinyStoriesV2-GPT4-train.txt\"\n",
    "vocab, merges = train_bpe(path_file, 10000, [\"<|endoftext|>\"])\n",
    "\n",
    "mem_after = process.memory_info().rss/1024/1024\n",
    "print(f\"Memory before: {mem_before} MB\")\n",
    "print(f\"Memory after: {mem_after} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' accomplishment'\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Find the longest token in the vocabulary\n",
    "\n",
    "max_vocab = max(vocab.values(), key=lambda x: len(x))\n",
    "print(max_vocab)\n",
    "print(len(max_vocab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverable: Implement a Tokenizer class that, given a vocabulary and a list of merges, encodes text into integer IDs and decodes integer IDs into text. Your tokenizer should also support user-provided special tokens (appending them to the vocabulary if they aren’t already there). We recommend the following interface:  \n",
    "\n",
    "`def __init__(self, vocab, merges, special_tokens=None)`\n",
    "\n",
    "Construct a tokenizer from a given vocabulary, list of merges, and (optionally) a list of special tokens. This function should accept the following parameters:  \n",
    "\n",
    "`vocab: dict[int, bytes]`  \n",
    "`merges: list[tuple[bytes, bytes]]`  \n",
    "`special_tokens: list[str] | None = None`  \n",
    "\n",
    "`def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)` \n",
    "\n",
    "Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges (in the same format that your BPE training code output) and (optionally) a list of special tokens. This method should accept the following additional parameters:  \n",
    "\n",
    "`vocab_filepath: str`  \n",
    "`merges_filepath: str`  \n",
    "`special_tokens: list[str] | None = None`  \n",
    "\n",
    "`def encode(self, text: str) -> list[int]` \n",
    "\n",
    "Encode an input text into a sequence of token IDs.  \n",
    "\n",
    "`def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]` \n",
    "\n",
    "Given an iterable of strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is required for memory-efficient tokenization of large files that we cannot directly load into memory.  \n",
    "\n",
    "`def decode(self, ids: list[int]) -> str` \n",
    "\n",
    "Decode a sequence of token IDs into text.  \n",
    "\n",
    "To test your Tokenizer against our provided tests, you will first need to implement the test adapter at [adapters.get_tokenizer]. Then, run pytest tests/test_tokenizer.py. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab:dict[int,bytes], merges:list[tuple[(bytes,bytes)]], special_tokens=None):\n",
    "        # load vocab \n",
    "        self.vocab = {}\n",
    "        self.vocab['int_to_byte'] = vocab \n",
    "        self.vocab['byte_to_int'] = {v:k for k,v in vocab.items()} \n",
    "\n",
    "        # load merges \n",
    "        self.merges = {}\n",
    "        for a, b in merges:\n",
    "            id_pair = (self.vocab['byte_to_int'][a], self.vocab['byte_to_int'][b])\n",
    "            self.merges[id_pair] = self.vocab['byte_to_int'][a+b]\n",
    "        \n",
    "        # load special tokens\n",
    "        self.special_tokens = {}\n",
    "        if special_tokens:\n",
    "            special_tokens = sorted(special_tokens, key=len, reverse=True)\n",
    "            for token in special_tokens:\n",
    "                token_bytes = token.encode(\"utf-8\")\n",
    "                if token_bytes not in self.vocab['byte_to_int']:\n",
    "                    self.vocab['int_to_byte'][len(self.vocab['int_to_byte'])] = token_bytes\n",
    "                    self.vocab['byte_to_int'][token_bytes] = len(self.vocab['int_to_byte']) \n",
    "                    self.special_tokens[token] = self.vocab['byte_to_int'][token_bytes]\n",
    "                else:\n",
    "                    self.special_tokens[token] = self.vocab['byte_to_int'][token_bytes]\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_file_path, merges_file_path, special_tokens=None):\n",
    "        vocab, merges = get_tokenizer_from_path(vocab_file_path, merges_file_path)\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text:str, progress_bar:bool=False)-> list[int]:\n",
    "        \"\"\"\n",
    "        Encode a text into token ids.\n",
    "        \"\"\"\n",
    "        if self.special_tokens:\n",
    "            chunk_pattern = \"(\" + \"|\".join(re.escape(token) for token in self.special_tokens) + \")\"\n",
    "            split_chunks = re.split(chunk_pattern, text)\n",
    "        else:\n",
    "            split_chunks = [text]\n",
    "        \n",
    "        ids = [] \n",
    "        for chunk in tqdm(split_chunks, disable=not progress_bar, desc=\"Encoding {len(split_chunks)} chunks\"):\n",
    "            new_ids = self.encode_chunk(chunk)\n",
    "            ids.extend(new_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode_chunk(self, chunk:str)-> list[int]:\n",
    "        \"\"\"\n",
    "        Encode a chunk of text into token ids.\n",
    "        \"\"\"\n",
    "        if chunk in self.special_tokens:\n",
    "            return [self.special_tokens[chunk]]\n",
    "        else:\n",
    "            tokens = re.findall(PAT, chunk)\n",
    "            total_ids = []\n",
    "            for token in tokens:\n",
    "                token_bytes = token.encode(\"utf-8\")\n",
    "                token_ids = [self.vocab['byte_to_int'][bytes([byte])] for byte in token_bytes]\n",
    "            \n",
    "                while len(token_ids) > 1:\n",
    "                    pairs = [(token_ids[i], token_ids[i+1]) for i in range(len(token_ids)-1)] # get all pairs of token_ids\n",
    "                    high_priority_pair = min(pairs, key=lambda x: self.merges.get(x, float('inf'))) # get the pair with the highest merge priority\n",
    "\n",
    "                    # We need to merge all instances of high_priority_pair in token_ids\n",
    "                    if high_priority_pair in self.merges: # if the pair is in merges, we merge\n",
    "                        new_token_id = self.merges[high_priority_pair]\n",
    "                        new_token_ids = []\n",
    "                        ind = 0\n",
    "                        while ind < len(token_ids): \n",
    "                            if ind < len(token_ids) - 1 and (token_ids[ind], token_ids[ind+1]) == high_priority_pair:\n",
    "                                new_token_ids.append(new_token_id)\n",
    "                                ind += 2\n",
    "                            else:\n",
    "                                new_token_ids.append(token_ids[ind])\n",
    "                                ind += 1\n",
    "                        token_ids = new_token_ids\n",
    "                    else: # if the pair is not in merges, we break\n",
    "                        break\n",
    "                total_ids.extend(token_ids)\n",
    "            return total_ids # return the token ids\n",
    "                \n",
    "\n",
    "\n",
    "    def encode_iterable(self, texts):\n",
    "        for text in texts:\n",
    "            ids = self.encode(text)\n",
    "            for id in ids:\n",
    "                yield id\n",
    "\n",
    "\n",
    "    def decode(self, ids: list[int])-> str:\n",
    "        text_bytes = b\"\".join([self.vocab['int_to_byte'][id] for id in ids])\n",
    "        return text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the example. Here \n",
    "\n",
    "```python\n",
    "vocab = {0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'th', 7: b' c', 8: b' a', 9: b'the', 10: b' at'}\n",
    "merges = [(b't', b'h'), (b' ', b'c'), (b' ', 'a'), (b'th', b'e'), (b' a', b't')]\n",
    "text = 'the cat ate'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 7, 1, 5, 10, 3]\n",
      "the cat ate\n"
     ]
    }
   ],
   "source": [
    "vocab = {0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'th', 7: b' c', 8: b' a', 9: b'the', 10: b' at'}\n",
    "merges = [(b't', b'h'), (b' ', b'c'), (b' ', b'a'), (b'th', b'e'), (b' a', b't')]\n",
    "text = 'the cat ate'\n",
    "tokenizer = Tokenizer(vocab, merges)\n",
    "# print(tokenizer.vocab['byte_to_int'])\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
