{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems for BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding Unicode\n",
    "- (a) What Unicode character does chr(0) return?  \n",
    "    Deliverable: A one-sentence response.  \n",
    "    '\\x00', space.\n",
    "- (b) How does this character’s string representation (__repr__()) differ from its printed representation?  \n",
    "    Deliverable: A one-sentence response.  \n",
    "    \"'\\x00'\", \" \"\n",
    "- (c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations: \n",
    "\n",
    "    Deliverable: A one-sentence response.\n",
    "    ```python\n",
    "    >>> chr(0) \n",
    "    >>> print(chr(0)) \n",
    "    >>> \"this is a test\" + chr(0) + \"string\"\n",
    "    >>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "    ```\n",
    "\n",
    "    In string, it's '\\x00'. If we print the string, it becomes space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0).__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Unicode Encodings\n",
    "- (a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "    - Vocabulary Size: UTF-8 limits the vocabulary to 256 possible tokens, making the model simpler.\n",
    "    - Efficiency: For texts with lots of ASCII characters, UTF-8 is much more space- and compute-efficient.\n",
    "    - Simplicity in Tokenization: Handling a stream of bytes is simpler than dealing with variable-length code units or surrogate pairs\n",
    "    - Robustness: Byte-level models trained on UTF-8 can better handle noisy or unexpected inputs.\n",
    "    -Compatibility: UTF-8’s ubiquity makes it easier to integrate with various data sources and systems.\n",
    "- (b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.  \n",
    "    ```python\n",
    "    def decode_utf8_bytes_to_str_wrong(bytestring: bytes): \n",
    "        return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])  \n",
    "    >>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")) \n",
    "    'hello'  \n",
    "    ``` \n",
    "    One token can has multiple bytes. For example \"蔡\".\n",
    "- (c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "    b'\\xe8\\x94'. Typically, they have some special bytes for start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe8\\x94\\xa1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\xe8\\x94'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes): return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "print(\"蔡\".encode(\"utf-8\"))\n",
    "b'\\xe8\\x94'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: BPE Tokenizer Training\n",
    "Deliverable: Write a function that, given path to an input text file, trains a (byte-level) BPE tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "- input_path: str Path to a text file with BPE tokenizer training data.  \n",
    "\n",
    "- vocab_size: int A non-negative integer that defines the maximum final vocabulary size (including the initial byte vocabulary, vocabulary items produced from merging, and any special tokens).  \n",
    "\n",
    "- special_tokens: list[str] A list of strings to add to the vocabulary. These special tokens do not otherwise affect BPE training.  \n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:  \n",
    "\n",
    "- vocab: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabulary) to bytes (token bytes).  \n",
    "\n",
    "- merges: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with <token2>. The merges should be ordered by order of creation.  \n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at [adapters.run_train_bpe]. Then, run pytest tests/test_train_bpe.py. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re \n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the code for BPE, we construct a bi-direct list, which will be very useful for the bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilist:\n",
    "    def __init__(self, val, id, previous=None, next=None):\n",
    "        self.val = val \n",
    "        self.id = id \n",
    "        self.prev = previous\n",
    "        self.next = next \n",
    "    \n",
    "    def merge(self):\n",
    "        if self.next is None:\n",
    "            raise ValueError(\"Cannot merge last element\")\n",
    "        else:\n",
    "            self.val = self.val + self.next.val\n",
    "            self.next = self.next.next\n",
    "            if self.next:\n",
    "                self.next.prev = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to write the prototype for the BPE algorithm. We consider some trivial cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'low', 'low', 'low', 'low', 'lower', 'lower', 'widest', 'widest', 'widest', 'newest', 'newest', 'newest', 'newest', 'newest', 'newest']\n",
      "[(b's', b't'), (b'e', b'st'), (b'o', b'w'), (b'l', b'ow'), (b'w', b'est'), (b'n', b'e')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "text = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest\" # text to be tokenized\n",
    "prim_tokens = text.split() # split text into tokens\n",
    "print(prim_tokens) # print tokens\n",
    "\n",
    "merges = [] # list of merges, tuple[bytes, bytes]\n",
    "vocab = {} # int to  bytes dictionary [int: bytes]\n",
    "\n",
    "id_to_token = {} # id to token dictionary\n",
    "token_to_id = {} # token to id dictionary\n",
    "frequency_tokens = {} # frequency of tokens table\n",
    "frequency_pairs = {}  # frequency of pairs table\n",
    "positions_pairs = defaultdict(list) # positions of pairs table, position is a bilist object\n",
    "\n",
    "\n",
    "# create token to id, id to token dictionaries, frequency_tokens table, and positions_pairs table\n",
    "for prim_token in prim_tokens:\n",
    "    chars = [bytes([b]) for b in prim_token.encode(\"utf-8\")] # get bytes of token\n",
    "    if prim_token in token_to_id:\n",
    "        frequency_tokens[token_to_id[prim_token]] += 1 # increment frequency of token\n",
    "    else:\n",
    "        id = len(token_to_id) # get new id\n",
    "        token_to_id[prim_token] = id # add token to id dictionary\n",
    "        id_to_token[id] = prim_token # add id to token dictionary\n",
    "        frequency_tokens[id] = 1 # set frequency of token to 1\n",
    "\n",
    "        bi_char = bilist(chars[0], id) # create first bilist object\n",
    "        for char in chars[1:]:\n",
    "            new_bi_char = bilist(char, id, bi_char) # create new bilist object\n",
    "            bi_char.next = new_bi_char # link new bilist object to previous one\n",
    "            pairs = (bi_char.val, new_bi_char.val) # create pair\n",
    "            positions_pairs[pairs].append(bi_char) # add position to pair\n",
    "            bi_char = new_bi_char # move to next bilist object\n",
    "\n",
    "# update the frequency_pairs table\n",
    "for pairs, positions in positions_pairs.items(): \n",
    "    for position in positions:\n",
    "        frequency_pairs[pairs] = frequency_pairs.get(pairs, 0) + frequency_tokens[position.id]  # increment frequency of pair\n",
    "\n",
    "# print(frequency_pairs)\n",
    "\n",
    "# find the pair with the highest frequency and marge \n",
    "for _ in range(6):\n",
    "    \n",
    "    max_pair = max(frequency_pairs, key=lambda x:(frequency_pairs[x], x)) # get pair with highest frequency\n",
    "    # print(max_pair) # print pair\n",
    "    merges.append(max_pair) # add pair to merges list\n",
    "    positions = positions_pairs[max_pair] # get positions of pair\n",
    "    \n",
    "    for position in positions: \n",
    "\n",
    "        val1 = position.val # get value of position\n",
    "        val2 = position.next.val # get value of next position\n",
    "        temp_pos =  position.next # get next position\n",
    "        position.merge() # merge position with next position\n",
    "\n",
    "        if position.prev: # if there is a previous position\n",
    "            prev_pair = (position.prev.val, val1)\n",
    "            frequency_pairs[prev_pair] -= frequency_tokens[position.id] # decrement frequency of pair   \n",
    "            positions_pairs[prev_pair].remove(position.prev) # remove previous position from pair\n",
    "            new_prev_pair = (position.prev.val, position.val) # create new pair\n",
    "            frequency_pairs[new_prev_pair] = frequency_pairs.get(new_prev_pair, 0) + frequency_tokens[position.id] # increment frequency of new pair\n",
    "            positions_pairs[new_prev_pair].append(position.prev) # add previous position to new pair\n",
    "\n",
    "        if position.next: # if there is a next position\n",
    "            next_pair = (val2, position.next.val)\n",
    "            frequency_pairs[next_pair] -= frequency_tokens[position.id] # decrement frequency of pair\n",
    "            positions_pairs[next_pair].remove(temp_pos) # remove next position from pair\n",
    "            new_next_pair = (position.val, position.next.val) # create new pair\n",
    "            frequency_pairs[new_next_pair] = frequency_pairs.get(new_next_pair, 0) + frequency_tokens[position.id] # increment frequency of new pair\n",
    "            positions_pairs[new_next_pair].append(position) # add position to new pair  \n",
    "\n",
    "    del positions_pairs[max_pair] # delete pair from positions_pairs\n",
    "    del frequency_pairs[max_pair] # delete pair from frequency_pairs\n",
    "\n",
    "    # print(frequency_pairs)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for now, this code is almost working. The remaining thing is to implement this for the general text. It's worth noting that for the general case, there is a special token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
